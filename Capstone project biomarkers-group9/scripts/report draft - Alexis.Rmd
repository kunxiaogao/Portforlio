---
title: "Report Draft - Alexis"
author: "Alexis Navarra"
date: '2022-10-31'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Use this as a template. Keep the headers and remove all other text. In all, your report can be quite short. When it is complete, render and then push changes to your team repository.

```{r}
# load any other packages and read data here
library(tidyverse)
library(tidymodels)
library(ISLR) 
library(glmnet) 
library(dplyr) 
library(tidyr)
```

# Abstract

Write a brief one-paragraph abstract that describes the contents of your write-up.

# Dataset

Write a brief data description, including: how data were obtained; sample characteristics; variables measured; and data preprocessing. This can be largely based on the source paper and should not exceed 1-2 paragraphs.

# Summary of published analysis

Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for GraphViz and Mermaid flowcharts.) Provide key results: the proteins selected for the classifier and the estimated accuracy.

Hewitson et al. used three computational methods in their study to find a panel of proteins with the highest predictive power for ASD. These three methods were: random-forest analysis, a t-test analysis, and a correction analysis with ASD severity. Each of these three methods produced a panel of 10 proteins with the highest predictive power for ASD. To optimize the predictive power of the model, they made the five proteins that were common to all three methods the 'core' proteins: MAPK14, IgD, DERM, EPHB2, and suPAR.

These five 'core' proteins were then used to train a prediction model to see if the remaining 13 'non-core' proteins provided any predictive power when added. Four additional proteins from this test were added to the 'core' proteins because their addition added predictive power to the model. The researchers also examined the impact of ethnicity, age, medications, and other clinical diagnoses to check if any confounding factors affected their results, but found no significant influences. Thus, the final panel of "optimal" proteins consisted of these nine proteins: MAPK14, IgD, DERM, EPHB2, suPAR, ROR1, GI24, e1F-4H, and ARSB.

# Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

# Impact of preprocessing and outliers

Tasks 1:

findings

Task 2:

# Methodlogical variations

Task 3

# Improved classifier

Task 4: Use any method to find either:

-   a simpler panel that achieves comparable classification accuracy,

-   or an alternative panel that achieves improved classification accuracy.

Benchmark your results against the in-class analysis.

To find a simpler/alternative panel of proteins, we decided to fit a lasso-regression model to the data to see which proteins were most influential to the predictive power of the model. A summary of our method to fit this model is as follows:

-   Partitioned the data into response and classifier training and testing data sets.

-   Fit a lasso-regression model and performed cross validation.

-   Found an optimal value of lambda and observed how it affected the MSE of our model's prediction (see plot below).

-   Displayed lasso regression coefficients associated with the best value of lambda (0.038).

Most of our lasso regression coefficients were zero. Any zero coefficients demonstrated that the associated protein did not have predictive power in our model. The non-zero coefficients are displayed below:

```{r, echo=FALSE}
# load in clean data
biomarker_clean_noAdos = biomarker_clean %>% select(-ados) 
biomarker <- biomarker_clean_noAdos %>%
  mutate(class = as.numeric(group == 'ASD'))

# partition
x <- biomarker %>%
  select(-group, -class) %>%
  as.matrix()
y <- biomarker %>%
  pull(class)

# create training and testing datasets
set.seed(1234)
train = sample(1:nrow(x), 123)
test = (-train)

x.train = x[train,]
y.train = y[train]
x.test = x[test,]
y.test = y[test]

# perform cross-validation to get test error
cv.out.lasso = cv.glmnet(x.train, y.train, alpha = 1) 
plot(cv.out.lasso)
abline(v = log(cv.out.lasso$lambda.min), col="red", lwd=3, lty=2)

# get best value of lambda and test mse
bestlam = cv.out.lasso$lambda.min

# get coefficient values from best lambda
out=glmnet(x,y,alpha=1,lambda = bestlam) 
coef <- coef(out)[,1]
coef_df <- as.data.frame(coef)
panel <- coef_df %>% filter(coef_df != 0)
panel
```

Thus, we found the above panel of 50 proteins as influential in predicting ASD. Because our MSE is relatively low, these findings are of comparible classification accuracy to the in-class analysis. Additionally, compared to the results of the in-class analysis, our panel is larger and much broader, which might be better in avoiding overfit of the data.
