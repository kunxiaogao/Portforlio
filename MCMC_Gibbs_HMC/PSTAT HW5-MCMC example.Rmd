---
title: "Homework 5"
author: "Kunxiao Gao, Xiao Zhang PSTAT 115, Fall 2022"
date: "Due on November 20, 2022 at 11:59 pm"
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
editor_options:
  markdown:
    wrap: 72
---


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo=TRUE, 
                      cache=FALSE, 
                      fig.width=5, 
                      fig.height=5,
                      fig.align='center')
r = function(x, digits=2){ round(x, digits=digits) }
indent1 = '    '      
indent2 = paste(rep(indent1, 2), collapse='')
library(tidyverse)
library(reshape2)
library(magrittr)
library(testthat)
library(rstan)
```

Q1.
(1a):
Since $Y_i|\lambda \sim Pois(\lambda)$, and $\lambda \sim Gamma(1, 0.25)$. Then $\lambda |Y_1,\cdots,Y_n \sim Gamma(1+\sum_{i=1}^ny_i, n+0.25)=Gamma(20, 5.25)$. Thus, the posterior mean is $a/b=\frac{1+4+7+3+2+3}{5+0.25}=\frac{20}{5.25}=3.8095$.

(1b):
```{r}
stan_model <-rstan::stan_model(file = "women_cup.stan")
set.seed(123)
stan_fit <- rstan::sampling(stan_model,data=list(N=5,y=c(4,7,3,2,3),refresh=0))
samples<- rstan::extract(stan_fit)
lambda_samples <- samples$lambda
summary(lambda_samples)
```
The estimated posterior mean is about 3.811.

(1c):
```{r}
x <- seq(0,7,length=1000)
hist(lambda_samples)
curve(dgamma(x,shape=20,rate=5.25)*2000,add = TRUE) #Notice that since the scale of frequency and posterior density are very different, to show better comparison between the histogram and the theoretical curve, we multiply the posterior density with 2000, which will still show the trend of the density curve. Thus, the curve in this graph is posterior density*2000.
```
The Monte Carlo samples coincide with the theoretical density.

(1d):
```{r}
y <- c()
for (i in lambda_samples) {
  yi <- rpois(1000,i)
  y <- append(y,yi)
}
mean(y)
```
The estimated mean of the predictive posterior distribution is about 3.8106.

Q2.
(2a):
```{r}
y <- c(70, 85, 111, 111, 115, 120, 123)
n <- length(y)
stan_model <-rstan::stan_model(file = "IQ_model.stan")
set.seed(123)
stan_fit <- rstan::sampling(stan_model,data=list(N=n,y=y,k0=1),refresh=0)
samples <- rstan::extract(stan_fit)
mu_samples <- samples$mu
sigma_samples <- samples$sigma
tibble(Mean=mu_samples,Precision=1/sigma_samples^2) %>%
  ggplot() + geom_point(aes(x=Mean,y=Precision)) + 
  theme_bw(base_size = 16)
```

(2b):
```{r}
sum(mu_samples > 100)/length(mu_samples)
```
The posterior probability that $\mu$ is greater than 100 is about 0.76975.

(2c):
```{r}
stan_model <-rstan::stan_model(file = "IQ_laplace_model_1.stan")
set.seed(123)
stan_fit2 <- rstan::sampling(stan_model,data=list(N=n,y=y,k0=1),refresh=0)
samples2 <- rstan::extract(stan_fit2)
mu_samples2 <- samples2$mu
sigma_samples2 <- samples2$sigma
sum(mu_samples2 > 100)/length(mu_samples2)
```
The posterior probability that the median IQ in the town is greater than 100 is about 0.943. It is much larger than the probability under the normal model. It makes sense because as we know the posterior is proportion to likelihood * prior: keeping the prior unchanged, we know that the normal distribution is not that appropriate because of the heavy tailed distribution of the observed data, and thus we will get a likelihood more weighted to left according to those two significantly lower values under normal model. In this case, the posterior distribution will also be shifted to left affected by its likelihood. However, since Laplace distribution has the property of heavy tail, Laplace model could more fit the data and thus we will get a likelihood not that weighted to left compared with the likelihood under normal model. Therefore, the posterior distribution should also more shift to right, which means the samples will become larger compared to the normal model. Thus, we will finally get a larger probability under Laplace model.

Q3.
(3a):
```{r}
cows_string = "
  data {
  int<lower=0> N1;
  int<lower=0> N2;
  vector[N1] y1;
  vector[N2] y2;
}

parameters {
  real mu1;
  real mu2;
  real<lower=0> sigma1;
  real<lower=0> sigma2;
}

model {
  sigma1 ~ uniform(0,1000);
  mu1 ~ uniform(0,2000);
  sigma2 ~ uniform(0,1000);
  mu2 ~ uniform(0,2000);
  y1 ~ normal(mu1, sigma1);
  y2 ~ normal(mu2, sigma2);
}
"
```

(3b):
```{r}
set.seed(123)
y1 <- c(651, 679, 374, 601, 401, 609, 767, 709, 704, 679)
n1 <- length(y1)
y2 <- c(798, 1139, 529, 609, 553, 743, 151, 544, 488, 555, 257, 692, 678, 675, 538)
n2 <- length(y2)
stan_samples = stan(model_code = cows_string,data = list(N1=n1,N2=n2,y1=y1,y2=y2))
```

(3c):
```{r}
samples <- rstan::extract(stan_samples)
mu_samples1 <- samples$mu1
sigma_samples1 <- samples$sigma1
mu_samples2 <- samples$mu2
sigma_samples2 <- samples$sigma2
stan_plot(stan_samples,ci_level=0.9,show_outer_line=FALSE)
```

(3d):
```{r}
mu_diff <- mu_samples2-mu_samples1
hist(mu_diff,breaks = 30)
```
It is not likely that the diet is going to make the cows produce more milk on average, since according to the histogram, mu_diff is largely distributed from 200 to 600, with the mean about 450. Therefore, it is very unlikely that $\mu_2$ < $\mu_1$.